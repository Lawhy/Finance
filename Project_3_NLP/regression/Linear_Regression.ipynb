{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model\n",
    "\n",
    "(Based on bag_of_words representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables initialisation\n",
    "with open('../stop_words/中文停用词表.txt', 'r', encoding='UTF-8-sig') as f:\n",
    "    stop_words = [ word.strip().replace('\\n', '') for word in f.readlines()]\n",
    "symbols = stop_words[0:26]\n",
    "print('e.g.', stop_words[23:33])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single input\n",
    "data_path = '请输入数据路径'  # e.g. data/record1.xls\n",
    "all_data = pd.read_excel(data_path)\n",
    "all_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple inputs\n",
    "# 如果能把所有的record数据都集成一个excel文件，就可以用上面的代码\n",
    "all_data = pd.DataFrame()\n",
    "for i in range(17):\n",
    "    i += 1\n",
    "    path = \"../data/record\" + str(i) + \".xls\"\n",
    "    all_data = all_data.append(pd.read_excel(path), sort=False)\n",
    "all_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取出想要的数据并且去除空行\n",
    "wanted_columns = ['文档号码','投资者关系活动主要内容介绍']\n",
    "data = all_data[wanted_columns]\n",
    "print('Before cleaning:')\n",
    "print(data.shape)\n",
    "data = data.dropna() # drop rows with null values\n",
    "print('After cleaning:')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理所需要的所有方法\n",
    "\n",
    "# clean the document, only Chinese characters, Numbers and Punctuations are left.\n",
    "def clean(doc):\n",
    "    chi = r'([\\u4E00-\\u9FA5]|[0-9]|[“”、。《》！，：；？\\.%])'\n",
    "    pa = re.compile(chi)\n",
    "    return \"\".join(re.findall(pa, doc))\n",
    "\n",
    "# sentence segmentation\n",
    "def sent_seg(cleaned_doc):\n",
    "    sent_pa = re.compile(r'.+?[？。！]')\n",
    "    return re.findall(sent_pa, cleaned_doc)\n",
    "\n",
    "def pure_sent(sent):\n",
    "    cleaned_sent_pa = re.compile(r'([\\u4E00-\\u9FA5])')\n",
    "    return ''.join(re.findall(cleaned_sent_pa, sent))\n",
    "        \n",
    "# Size of a doc is defined as the total number of valid Chinese characters\n",
    "def raw_process(doc):\n",
    "    cleaned_doc = clean(doc)\n",
    "    sents = sent_seg(cleaned_doc)\n",
    "    if not cleaned_doc or not len(sents):\n",
    "        return {\n",
    "            'sents': [],\n",
    "            'size': 0,\n",
    "            'avg_sent_len' : 0\n",
    "        }\n",
    "    else:\n",
    "        total_length = sum([len(pure_sent(sent)) for sent in sents])\n",
    "        avg_sent_length = total_length / len(sents)\n",
    "        return {\n",
    "            'sents': sents,\n",
    "            'size' : total_length,\n",
    "            'avg_sent_len' : avg_sent_length \n",
    "        }\n",
    "\n",
    "# generate frequency distribution for each document, vital step for bag_of_words representation\n",
    "def gen_freq_dist(doc):\n",
    "    stat = raw_process(doc)\n",
    "    sents = stat['sents']\n",
    "    freq_dist = dict()\n",
    "    pa = re.compile(r'([$0123456789?_“”、。《》！，：；？\\.%])')\n",
    "    for sent in sents:\n",
    "        # calculate sent length after\n",
    "        words = jieba.cut(sent, cut_all=False, HMM=True)\n",
    "        for word in words:\n",
    "            # ignore all the stop words\n",
    "            if (not word in stop_words) and (not re.findall(pa, word)):\n",
    "                freq_dist.setdefault(word, 0)\n",
    "                freq_dist[word] += 1\n",
    "    return { 'freq_dist' : freq_dist, \n",
    "             'size' : stat['size'],\n",
    "             'avg_sent_len' : stat['avg_sent_len'],\n",
    "             'n_sents' : len(sents)\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full frequencey distribution\n",
    "\n",
    "1. 如果已经对当前的data完整运行过readability，请将readability文件夹里面的all_freq_dist.json复制到regression文件夹里，从而复用数据，并使用下数第二个cell进行读取。\n",
    "2. 如果尚未生成当前data的完整freq_dist，请跑一次下数第一个cell（One-time-block）进行生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-time block \n",
    "# 建立一个完整的 frequency distribution，推荐只跑一次将数据储存以复用\n",
    "def init_all_freq_dist():\n",
    "    count = 0\n",
    "    all_freq_dist = dict()\n",
    "    for index, d in data.iterrows():\n",
    "        print('[' + str(count) + '] Processing document ' + str(d['文档号码']) + '...')\n",
    "        fd = gen_freq_dist(d[1])['freq_dist']\n",
    "        for k in fd.keys():\n",
    "            all_freq_dist.setdefault(k, 0)\n",
    "            all_freq_dist[k] += fd[k]\n",
    "        count += 1\n",
    "    return all_freq_dist\n",
    "\n",
    "all_freq_dist = init_all_freq_dist()\n",
    "with open('all_freq_dist.json', 'w+', encoding='UTF-8-sig') as f:\n",
    "    json.dump(all_freq_dist, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果前一个cell已经完整跑完一次，只需要跑这个cell就能拿到完整的 frequency distribution\n",
    "with open('all_freq_dist.json', 'r', encoding='UTF-8-sig') as f:\n",
    "    all_freq_dist = json.load(f)\n",
    "all_freq_dist_df = pd.DataFrame.from_dict(all_freq_dist, orient='index', columns=['freq'])\n",
    "print('Most frequent word is: ' + str(np.argmax(all_freq_dist_df['freq'])))\n",
    "all_freq_dist_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words construction\n",
    "\n",
    "有了完整的freq_dist以后就能把每个document都转换成bag_of_words形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先把frequency低于3的全部去掉\n",
    "low_freq_words = [word for word in all_freq_dist.keys() if all_freq_dist[word] <= 3]\n",
    "for lw in low_freq_words:\n",
    "    del all_freq_dist[lw]\n",
    "print('Remaining number of words:', len(all_freq_dist.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()\n",
    "all_bow = vec.fit_transform(all_freq_dist).toarray()\n",
    "print('e.g.', vec.get_feature_names()[12000:12010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_bag_of_words(doc):\n",
    "    global vec\n",
    "    return vec.transform(gen_freq_dist(doc)['freq_dist']).toarray()\n",
    "\n",
    "def all_bag_of_words(col_name_for_docs, limited=False):\n",
    "    global vec, data\n",
    "    dimension = len(vec.get_feature_names())\n",
    "    count = 0\n",
    "    init = True\n",
    "    X = []\n",
    "    for index, data_point in data.iterrows():\n",
    "        # 文档号码如果不存在将以下print注释，或者替换成另外指明数据的列\n",
    "        print('[' + str(count) + '] Transforming document ' + str(data_point['文档号码']) + '...')\n",
    "        # Initialise X with first document\n",
    "        if init:\n",
    "            X = gen_bag_of_words(data_point[col_name_for_docs])\n",
    "            init = False\n",
    "        else:\n",
    "            X = np.vstack((X, gen_bag_of_words(data_point[col_name_for_docs])))\n",
    "        count += 1\n",
    "        \n",
    "        if limited and count == 1000:\n",
    "            # For test use, just use the first 1000 rows\n",
    "            break\n",
    "    \n",
    "    return X\n",
    "\n",
    "# returning the coeffcients of the linear regression model after fitting X and y\n",
    "def lr_coeffs(X, y):\n",
    "    global vec\n",
    "    features = list(vec.get_feature_names())\n",
    "    reg = LinearRegression().fit(X, y)\n",
    "    coeffs = list(reg.coef_)\n",
    "    result = pd.DataFrame(columns=['Feature', 'Coefficients'])\n",
    "    result['Feature'] = features\n",
    "    result['Coefficients'] = coeffs\n",
    "    return result.sort_values(by=['Coefficients'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply for testing\n",
    "test_X = all_bag_of_words('投资者关系活动主要内容介绍', limited=True)\n",
    "test_y = np.dot(X, np.array([1, 2] * int(59536/2))) + 3\n",
    "dummy = pd.DataFrame()\n",
    "dummy['Y'] = list(test_y)\n",
    "dummy['文档号码'] = list(data['文档号码'][:1000])\n",
    "dummy.to_excel('dummy_Y.xlsx')\n",
    "test_result = lr_coeffs(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们需要读取真正的Y值，格式为excel文件，且仅有两列，一列是ID（比如文档号码），一列是Y值。\n",
    "\n",
    "$\\textbf{注意！}$ 读取Y值的文件里，ID的对应顺序要和提供训练数据的文档ID一致！一个简单的办法就是把Y值先按ID添加到原数据中，再进行分割即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Y(file_path, col_name_ID, col_name_Y):\n",
    "    global data\n",
    "    df = pd.read_excel(file_path)\n",
    "    if list(df[col_name_ID]) == list(data[col_name_ID]):\n",
    "        print('训练数据与Y值的文档ID成功匹配！')\n",
    "        return np.array(list(df[col_name_Y]))\n",
    "    else:\n",
    "        print('警告！训练数据与Y值的文档ID不匹配，请检查！')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_path = '请替换成储存Y值文件的路径' # e.g. dummy_Y.xlsx\n",
    "doc_ID_name = '请替换文档ID的名称' # e.g. 文档号码\n",
    "y_name = '请替换Y值的名称' \n",
    "y = load_Y(Y_path, doc_ID_name, Y_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = lr_coeffs(X, y)\n",
    "# 储存结果\n",
    "result.to_excel('word_ranking.xlsx')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
