{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. Packages and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1: Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from nltk.probability import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2: Read data\n",
    "\n",
    "#### （代码效果与sentiment analysis里的一样） "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single input\n",
    "data_path = '请输入数据路径'  # e.g. data/record1.xls\n",
    "all_data = pd.read_excel(data_path)\n",
    "all_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple inputs\n",
    "all_data = pd.DataFrame()\n",
    "for i in range(17):\n",
    "    i += 1\n",
    "    path = \"data/record\" + str(i) + \".xls\"\n",
    "    all_data = all_data.append(pd.read_excel(path), sort=False)\n",
    "all_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = all_data[['文档号码','投资者关系活动主要内容介绍']]\n",
    "print('Before cleaning:')\n",
    "print(data.shape)\n",
    "data = data.dropna()\n",
    "print('After cleaning:')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Preprocessing\n",
    "### 1.1 Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the error docs after second-level cleaning\n",
    "def clean(doc):\n",
    "    chi = r'([\\u4E00-\\u9FA5]|[0-9]|[“”、。《》！，：；？\\.%])'\n",
    "    pa = re.compile(chi)\n",
    "    return \"\".join(re.findall(pa, doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 以下两个cells只是可视化cleaning和分词的效果，可以忽略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean(data.iloc[0][1]))\n",
    "print(\"--------------\")\n",
    "print(data.iloc[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_list = jieba.cut(clean(data.iloc[0][1][0:500]), cut_all=False, HMM=True)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list))  # 默认模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Split into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_seg(cleaned_doc):\n",
    "    sent_pa = re.compile(r'.+?[？。！]')\n",
    "    return re.findall(sent_pa, cleaned_doc)\n",
    "\n",
    "def pure_sent(sent):\n",
    "    cleaned_sent_pa = re.compile(r'([\\u4E00-\\u9FA5])')\n",
    "    return ''.join(re.findall(cleaned_sent_pa, sent))\n",
    "        \n",
    "# Size of a doc is defined as the total number of valid Chinese characters\n",
    "def raw_process(doc):\n",
    "    cleaned_doc = clean(doc)\n",
    "    sents = sent_seg(cleaned_doc)\n",
    "    if not cleaned_doc or not len(sents):\n",
    "        return {\n",
    "            'sents': [],\n",
    "            'size': 0,\n",
    "            'avg_sent_len' : 0\n",
    "        }\n",
    "    else:\n",
    "        total_length = sum([len(pure_sent(sent)) for sent in sents])\n",
    "        avg_sent_length = total_length / len(sents)\n",
    "        return {\n",
    "            'sents': sents,\n",
    "            'size' : total_length,\n",
    "            'avg_sent_len' : avg_sent_length \n",
    "        }\n",
    "    \n",
    "raw_process(data.iloc[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Stop word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../stop_words/中文停用词表.txt', 'r', encoding='UTF-8-sig') as f:\n",
    "    stop_words = [ word.strip().replace('\\n', '') for word in f.readlines()]\n",
    "symbols = stop_words[0:26]\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Generating frequency distribution \n",
    "(using stop word list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_freq_dist(doc):\n",
    "    stat = raw_process(doc)\n",
    "    sents = stat['sents']\n",
    "    freq_dist = dict()\n",
    "    pa = re.compile(r'([$0123456789?_“”、。《》！，：；？\\.%])')\n",
    "    for sent in sents:\n",
    "        # calculate sent length after\n",
    "        words = jieba.cut(sent, cut_all=False, HMM=True)\n",
    "        for word in words:\n",
    "            # ignore all the stop words\n",
    "            if (not word in stop_words) and (not re.findall(pa, word)):\n",
    "                if not word in freq_dist.keys():\n",
    "                    freq_dist[word] = 1\n",
    "                else:\n",
    "                    freq_dist[word] += 1\n",
    "    return { 'freq_dist' : freq_dist, \n",
    "             'size' : stat['size'],\n",
    "             'avg_sent_len' : stat['avg_sent_len'],\n",
    "             'n_sents' : len(sents)\n",
    "           }\n",
    "\n",
    "gen_freq_dist(data.iloc[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Compute the readability in different ways\n",
    "\n",
    "### 2.1 Grade and Semester \n",
    "(using common word list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_pa = re.compile(r'([\\u4E00-\\u9FA5])')\n",
    "with open('../common_words/常用词.txt', 'r', encoding='UTF-8-sig') as f:\n",
    "    common_words = [ ''.join(re.findall(ch_pa, line)) for line in f.readlines()]\n",
    "print(len(common_words))\n",
    "print('为了' in common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{aligned}\n",
    "年级 &= 17.52547988 + 0.00242523 \\times 课文长度 \\\\\n",
    "      &+ 0.04414527 \\times 平均句长 - 18.33435443 * 常用字比率\n",
    "\\end{aligned}$\n",
    "\n",
    "$\\begin{aligned}\n",
    "学期 &= 34.53858379 + 0.00491625 \\times 课文长度 \\\\\n",
    "      &+ 0.08996394 \\times 平均句长 - 36.73710603 * 常用字比率\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_n_semester(doc):\n",
    "    stat = gen_freq_dist(doc)\n",
    "    freq_dist = stat['freq_dist']\n",
    "    n_words = sum(freq_dist.values())\n",
    "    \n",
    "    # compute the percentage of common words\n",
    "    n_common_words = 0\n",
    "    for word in freq_dist.keys():\n",
    "        if word in common_words:\n",
    "            n_common_words += freq_dist[word]\n",
    "    \n",
    "    # all the required statistics\n",
    "    article_length = stat['size']\n",
    "    avg_sent_len = stat['avg_sent_len']\n",
    "    \n",
    "    # for empty doc\n",
    "    if not article_length or not avg_sent_len:\n",
    "        return {\n",
    "            'grade' : None,\n",
    "            'semester' : None,\n",
    "            'common_words_percentage' : None\n",
    "        }\n",
    "    \n",
    "    percent_common_words = n_common_words / n_words\n",
    "    \n",
    "    # compute grade & semester\n",
    "    grade = 17.52547988 + 0.00242523 * article_length \\\n",
    "            + 0.04414527 * avg_sent_len - 18.33435443 * percent_common_words\n",
    "    semester = 34.53858379 + 0.00491625 * article_length \\\n",
    "             + 0.08996394 * avg_sent_len - 36.73710603 * percent_common_words\n",
    "    \n",
    "    return {\n",
    "        'grade' : grade,\n",
    "        'semester' : semester,\n",
    "        'common_words_percentage' : percent_common_words\n",
    "    }\n",
    "\n",
    "grade_n_semester(data.iloc[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Fog value and its modified version \n",
    "(using the full frequency distribution (all_freq_dist) and the document frequency distribution (df_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-time block \n",
    "# 建立一个完整的 frequency distribution，推荐只跑一次将数据储存以复用\n",
    "def init_all_freq_dist():\n",
    "    all_freq_dist = dict()\n",
    "    count = 0\n",
    "    for index, d in data.iterrows():\n",
    "        print('[' + str(count) + '] Processing document ' + str(d['文档号码']) + '...')\n",
    "        fd = gen_freq_dist(d[1])['freq_dist']\n",
    "        for k in fd.keys():\n",
    "            if k not in all_freq_dist.keys():\n",
    "                all_freq_dist[k] = fd[k]\n",
    "            else:\n",
    "                all_freq_dist[k] += fd[k]\n",
    "        count += 1\n",
    "    return all_freq_dist\n",
    "\n",
    "all_freq_dist = init_all_freq_dist()\n",
    "with open('all_freq_dist.json', 'w+', encoding='UTF-8-sig') as f:\n",
    "    json.dump(all_freq_dist, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果前一个cell已经完整跑完一次，只需要跑这个cell就能拿到完整的 frequency distribution\n",
    "with open('all_freq_dist.json', 'r', encoding='UTF-8-sig') as f:\n",
    "    all_freq_dist = json.load(f)\n",
    "\n",
    "all_freq_dist_df = pd.DataFrame.from_dict(all_freq_dist, orient='index', columns=['freq'])\n",
    "print('Most frequent word is: ' + str(np.argmax(all_freq_dist_df['freq'])))\n",
    "all_freq_dist_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the summary here, apparently defining 5% or 10% as rare/complex words is not appropriate, so instead, define complex words to be frequency less than 1 or 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_complex(word, threshold=1):\n",
    "    global all_freq_dist\n",
    "    return all_freq_dist[word] <= threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-time block\n",
    "# 建立一个完整的 document frequency distribution，推荐只跑一次将数据储存以复用\n",
    "# df的定义在 10-K readability 那篇paper里\n",
    "def init_df_dist():\n",
    "    df_dist = dict()\n",
    "    count = 0\n",
    "    for index, d in data.iterrows():\n",
    "        print('[' + str(count) + '] Processing document ' + str(d['文档号码']) + '...')\n",
    "        fd = gen_freq_dist(d[1])['freq_dist']\n",
    "        for k in fd.keys():\n",
    "            df_dist.setdefault(k, 0)\n",
    "            df_dist[k] += 1\n",
    "        count += 1\n",
    "    return df_dist\n",
    "\n",
    "df_dist = init_df_dist()\n",
    "with open('df_dist.json', 'w+', encoding='UTF-8-sig') as f:\n",
    "    json.dump(df_dist, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df_dist.json', 'r', encoding='UTF-8-sig') as f:\n",
    "    df_dist = json.load(f)\n",
    "\n",
    "df_dist_df = pd.DataFrame.from_dict(df_dist, orient='index', columns=['df'])\n",
    "print('Most diverse word is: ' + str(np.argmax(df_dist_df['df'])))\n",
    "df_dist_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-K readability paper 里定义的词比重\n",
    "def weight_of_word(word):\n",
    "    global data, df_dist\n",
    "    N = data.shape[0]\n",
    "    df = df_dist[word]\n",
    "    return np.log(N / df) / np.log(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fog(word_per_sent, percent_cw):\n",
    "    return 0.4 * (word_per_sent + 100 * percent_cw)\n",
    "\n",
    "def fog_all(doc):\n",
    "    global all_freq_dist\n",
    "    stat = gen_freq_dist(doc)\n",
    "    freq_dist = stat['freq_dist']\n",
    "    n_words = sum(freq_dist.values())\n",
    "    n_sents = stat['n_sents']\n",
    "    \n",
    "    # for empty doc\n",
    "    if not n_sents or not n_words:\n",
    "        return {\n",
    "            'original_fog_t1' : None,\n",
    "            'original_fog_t3' : None,\n",
    "            'weighted_fog_t1' : None,\n",
    "            'weighted_fog_t3' : None\n",
    "        }\n",
    "    \n",
    "    # complex words percentage\n",
    "    percent_cw_dict = {\n",
    "        'o_t1' : (sum([freq_dist[word] for word in freq_dist.keys() if is_complex(word, threshold=1)]) / n_words),\n",
    "        'o_t3' : (sum([freq_dist[word] for word in freq_dist.keys() if is_complex(word, threshold=3)]) / n_words),\n",
    "        'w_t1' : (sum([(weight_of_word(word) * freq_dist[word]) \\\n",
    "                               for word in freq_dist.keys() if is_complex(word, threshold=1)]) / n_words),\n",
    "        'w_t3' : (sum([(weight_of_word(word) * freq_dist[word]) \\\n",
    "                               for word in freq_dist.keys() if is_complex(word, threshold=3)]) / n_words)\n",
    "    }\n",
    "    \n",
    "    word_per_sent = n_words / n_sents\n",
    "    \n",
    "    return {\n",
    "            'original_fog_t1' : fog(word_per_sent, percent_cw_dict['o_t1']),\n",
    "            'original_fog_t3' : fog(word_per_sent, percent_cw_dict['o_t3']),\n",
    "            'weighted_fog_t1' : fog(word_per_sent, percent_cw_dict['w_t1']),\n",
    "            'weighted_fog_t3' : fog(word_per_sent, percent_cw_dict['w_t3'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = pd.DataFrame(columns=['文档号码', '总长度', '平均句长', '常用词比例', '年级', '学期', 'original_fog_t1', 'original_fog_t3', 'weighted_fog_t1', 'weighted_fog_t3'])\n",
    "count = 0\n",
    "error_docs = []\n",
    "for index, d in data.iterrows():\n",
    "    print('[' + str(count) + '] Processing document ' + str(d['文档号码']) + '...')\n",
    "    count += 1\n",
    "    doc = d['投资者关系活动主要内容介绍']\n",
    "    doc_num = d['文档号码']\n",
    "    scores = pd.DataFrame(columns=['文档号码', '总长度', '平均句长', '常用词比例', '年级', '学期', 'original_fog_t1', 'original_fog_t3', 'weighted_fog_t1', 'weighted_fog_t3'])\n",
    "    scores['文档号码'] = [doc_num]\n",
    "    try:\n",
    "        raw_stat = raw_process(doc)\n",
    "        scores['总长度'] = [raw_stat['size']]\n",
    "        scores['平均句长'] = [raw_stat['avg_sent_len']]\n",
    "        # 学期,年级\n",
    "        result_1 = grade_n_semester(doc)\n",
    "        scores['常用词比例'] = [result_1['common_words_percentage']]\n",
    "        scores['年级'] = [result_1['grade']]\n",
    "        scores['学期'] = [result_1['semester']]\n",
    "        # fog\n",
    "        result_2 = fog_all(doc)\n",
    "        scores['original_fog_t1'] = [result_2['original_fog_t1']]\n",
    "        scores['original_fog_t3'] = [result_2['original_fog_t3']]\n",
    "        scores['weighted_fog_t1'] = [result_2['weighted_fog_t1']]\n",
    "        scores['weighted_fog_t3'] = [result_2['weighted_fog_t3']]\n",
    "        # appending\n",
    "        all_scores = all_scores.append(scores)\n",
    "    except:\n",
    "        error_docs.append(doc_num)\n",
    "        print('An error happened when processing document ' + str(doc_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['文档号码'].isin(error_docs)] #看还有没有无法处理的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores.to_excel('readability/readability.xlsx', index=False) #储存最终结果"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
