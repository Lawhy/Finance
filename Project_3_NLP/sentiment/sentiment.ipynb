{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "### (using Baidu Brain's NLP tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import necessary pacakages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aip import AipNlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: 百度大脑使用授权 (如果失效可自行注册一个账号，按照官网的指示拿到以下三个值）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important setup for Baidu Brain's NLP\n",
    "APP_ID = \"16323168\"\n",
    "API_KEY = \"azMXa7UuqFH8qsXTsumh1XoF\"\n",
    "SECRET_KEY = \"CfYjxNl4SMAkorMpXIhAclyIQ3nAzwE9\"\n",
    "\n",
    "client = AipNlp(APP_ID, API_KEY, SECRET_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1: 单一输入文件请用以下代码 （输入excel文件，尾缀是xls或者xlsx）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single input\n",
    "data_path = '请输入数据路径'  # e.g. data/record1.xls\n",
    "all_data = pd.read_excel(data_path)\n",
    "all_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: 多文件输入（由于这次给的数据命名为 recordN.xls 的形式， N为1到17） 所以用一个for-loop 把所有数据读取）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple inputs\n",
    "all_data = pd.DataFrame()\n",
    "for i in range(17):\n",
    "    i += 1\n",
    "    path = \"../data/record\" + str(i) + \".xls\"\n",
    "    all_data = all_data.append(pd.read_excel(path), sort=False)\n",
    "all_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: 只取数据中有用的列，并且去掉所有数据残缺的行, dropna()的功能就是去掉具有空值的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = all_data[['文档号码','投资者关系活动主要内容介绍']] # 如果需要不同名字的列，请修改中括号里面的内容，以逗号分隔\n",
    "print('Before cleaning:')\n",
    "print(data.shape)\n",
    "data = data.dropna()\n",
    "print('After cleaning:')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Sentiment Analysis Method 封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate results for one document after cleaning\n",
    "def analyse(doc, doc_num):\n",
    "    # dividing\n",
    "    set_of_docs = []\n",
    "    num_of_portions = int((len(doc) / 1000)) + 1\n",
    "    for i in range(num_of_portions):\n",
    "        sub_doc = doc[i*1000:(i+1)*1000]\n",
    "        if not i == num_of_portions - 1:\n",
    "            assert len(sub_doc) == 1000\n",
    "        else:\n",
    "            assert len(sub_doc) <= 1000\n",
    "        set_of_docs.append(sub_doc)\n",
    "    # sentiment analysis for each sub_doc\n",
    "    results = []\n",
    "    for sub_doc in set_of_docs:\n",
    "        if not sub_doc:\n",
    "            break\n",
    "        result_dict = client.sentimentClassify(sub_doc)['items'][0]\n",
    "        result_dict['weight'] = len(sub_doc) / len(doc)\n",
    "        results.append(result_dict)\n",
    "        \n",
    "    # init the scores\n",
    "    scores = pd.DataFrame()\n",
    "    scores['文档号码'] = [doc_num]\n",
    "    \n",
    "    # deal with empty results\n",
    "    if not results:\n",
    "        scores['positive_prob'] = None\n",
    "        scores['negative_prob'] = None\n",
    "        scores['sentiment'] = None\n",
    "        scores['positive_prob_c'] = None\n",
    "        scores['negative_prob_c'] = None\n",
    "        scores['sentiment_c'] = None\n",
    "        return scores\n",
    "    \n",
    "    # transfer to the wanted scores\n",
    "    scores['positive_prob'] = sum([ (r_dict['positive_prob'] * r_dict['weight']) for r_dict in results ])\n",
    "    scores['negative_prob'] = sum([ (r_dict['negative_prob'] * r_dict['weight']) for r_dict in results ])\n",
    "    scores['sentiment'] = round(sum([ (r_dict['sentiment'] * r_dict['weight']) for r_dict in results ]))\n",
    "    scores['positive_prob_c'] = sum([ (r_dict['positive_prob'] * r_dict['weight'] * r_dict['confidence']) for r_dict in results ])\n",
    "    scores['negative_prob_c'] = sum([ (r_dict['negative_prob'] * r_dict['weight'] * r_dict['confidence']) for r_dict in results ])\n",
    "    scores['sentiment_c'] = round(sum([ (r_dict['sentiment'] * r_dict['weight'] * r_dict['confidence']) for r_dict in results ]))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: 定义clean_error_doc方法来将无法完整分析的error_docs二次处理，只留下中文，数字和标点符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the error docs after second-level cleaning\n",
    "def clean_error_doc(error_doc):\n",
    "    chi = r'([\\u4E00-\\u9FA5]|[0-9]|[“”、。《》！，：；？\\.%])'\n",
    "    pa = re.compile(chi)\n",
    "    return \"\".join(re.findall(pa, error_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Sentiment Analysis 执行 （运行时间根据数据多少而定）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = pd.DataFrame()\n",
    "error_docs = []\n",
    "count = 0\n",
    "for index, d in data.iterrows():\n",
    "    print('[' + str(count) + '] Processing document ' + str(d['文档号码']) + '...')\n",
    "    count += 1\n",
    "    doc = d['投资者关系活动主要内容介绍']\n",
    "    doc_num = d['文档号码']\n",
    "    # cleaning\n",
    "    doc = doc.replace('\\n', '')\n",
    "    doc = doc.replace(' ', '')\n",
    "    doc = doc.replace('\\xa0', '')\n",
    "    doc = doc.replace('\\t', '')\n",
    "    doc = doc.replace('\\uf0a1', '')\n",
    "    \n",
    "    try:\n",
    "        scores = analyse(doc, str(doc_num))\n",
    "        all_scores = all_scores.append(scores)\n",
    "    except:\n",
    "        error_docs.append(str(doc_num))\n",
    "        print('An error happened when processing document ' + str(doc_num) + ', apply cleaning...')\n",
    "        # 无法处理的文件进行cleaning后再处理\n",
    "        doc = clean_error_doc(doc)\n",
    "        try:\n",
    "            scores = analyse(doc, str(doc_num))\n",
    "            all_scores = all_scores.append(scores)\n",
    "        except:\n",
    "            error_docs.append(str(doc_num))\n",
    "            print('Cleaning failed, the document ' + str(doc_num) + ' cannot be processed...')\n",
    "            scores = pd.DataFrame()\n",
    "            scores['文档号码'] = [doc_num]\n",
    "            scores['positive_prob'] = None\n",
    "            scores['negative_prob'] = None\n",
    "            scores['sentiment'] = None\n",
    "            scores['positive_prob_c'] = None\n",
    "            scores['negative_prob_c'] = None\n",
    "            scores['sentiment_c'] = None\n",
    "            all_scores = all_scores.append(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: 结果储存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores.to_excel('sentiment.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
